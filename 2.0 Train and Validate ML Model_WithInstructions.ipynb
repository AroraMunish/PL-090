{"cells":[{"cell_type":"markdown","source":["# Training and Validating a Machine Learning Model\n\nLinear regression is the most commonly employed machine learning model since it is highly interpretable and well studied.  This is often the first pass for data scientists modeling continuous variables.  This notebook trains a multivariate regression model and interprets the results. This notebook is organized in two sections:\n\n- Exercise 1: Training a Model\n- Exercise 2: Validating a Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f80712-2461-4fcf-9fce-9348e7d014d5"}}},{"cell_type":"markdown","source":["Run the following cell to load common libraries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22fda893-2242-45e6-844b-1844109462fb"}}},{"cell_type":"code","source":["import urllib.request\nimport os\nimport numpy as np\nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import col, lit\nfrom pyspark.sql.functions import udf\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(\"Imported common libraries.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a033b72b-f6a7-411e-869a-e17d649c836f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Load the training data\n\nIn this notebook, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets]( https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. Each row of the table represents a taxi ride that includes columns such as number of passengers, trip distance, datetime information, holiday and weather information, and the taxi fare for the trip.\n\nRun the following cell to load the table into a Spark dataframe and reivew the dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77e8a28-e267-4775-92e2-57025e637213"}}},{"cell_type":"code","source":["dataset = spark.sql(\"select * from nyc_taxi\")\ndisplay(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e218c3ef-5a41-4d5c-873c-46934a1c9991"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Exercise 1: Training a Model\n\nIn this section we will use the Spark's machine learning library, `MLlib` to train a `NYC Taxi Fare Predictor` machine learning model. We will train a multivariate regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information. Before we start, let's review the three main abstractions that are provided in the `MLlib`:<br><br>\n\n1. A **transformer** takes a DataFrame as an input and returns a new DataFrame with one or more columns appended to it.  \na. Transformers implement a **`.transform()`** method.\n2. An **estimator** takes a DataFrame as an input and returns a model, which itself is a transformer.  \na. Estimators implements a **`.fit()`** method.\n3. A **pipeline** combines together transformers and estimators to make it easier to combine multiple algorithms.  \na. Pipelines implement a **`.fit()`** method.\n  \nThese basic building blocks form the machine learning process in Spark from featurization through model training and deployment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e257d98-a8b0-4709-ad9a-b75c615a033d"}}},{"cell_type":"markdown","source":["### Featurization of the training data\n\nMachine learning models are only as strong as the data they see and can only work on numerical data.  \n**Featurization is the process of creating this input data for a model.**  \nIn this section we will build derived features and create a pipeline of featurization steps."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3cb471d-0f0d-4806-8dfe-5d6103d6d41a"}}},{"cell_type":"markdown","source":["Run the following cell to engineer the cyclical features to represent `hour_of_day`. Also, we will drop rows with null values in the `totalAmount` column and convert the column ` isPaidTimeOff ` as integer type."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81f39d63-5392-4a92-92f4-9fa3353a4c2c"}}},{"cell_type":"code","source":["def get_sin_cosine(value, max_value):\n  sine =  np.sin(value * (2.*np.pi/max_value))\n  cosine = np.cos(value * (2.*np.pi/max_value))\n  return (sine.tolist(), cosine.tolist())\n\n# Third parameter in StructField specifies if the field can be nullable.\nschema = StructType([\n    StructField(\"sine\", DoubleType(), False),\n    StructField(\"cosine\", DoubleType(), False)\n])\n\nget_sin_cosineUDF = udf(get_sin_cosine, schema)\n\n# Create dataset with columns hour_sine and hour_cosine and drop the column hour_of_day\ndataset = dataset.withColumn(\"udfResult\", get_sin_cosineUDF(col(\"hour_of_day\"), lit(24))).withColumn(\"hour_sine\", col(\"udfResult.sine\")).withColumn(\"hour_cosine\", col(\"udfResult.cosine\")).drop(\"udfResult\").drop(\"hour_of_day\")\n\ndataset = dataset.filter(dataset.totalAmount.isNotNull())\n\n#Cast field isPaidTimeOff to integer\ndataset = dataset.withColumn(\"isPaidTimeOff\", col(\"isPaidTimeOff\").cast(\"integer\"))\n\ndisplay(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6128f4f4-9378-4c6e-ae9f-ed423596d4dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the following cell to create stages in our featurization pipeline to scale the numerical features and to encode the categorical features."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"090891ff-9817-4098-83fa-1bc00c87718c"}}},{"cell_type":"markdown","source":["###Important points to note###\n•\t**Imputer:** The imputer estimator is fit on the dataset to calculate the statistic for each column. The fit imputer is then applied to the dataset to create a copy of the dataset with all the missing values for each column replaced with the calculated mean statistic.\n\n•\t**VectorAssembler:** VectorAssembler is a transformer that combines a given list of columns into a single vector column. \nIt is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees\n\n•\t**MinMaxScaler:** Transform features by scaling each feature to a given range.\nA way to normalize the input features/variables is the Min-Max scaler. By doing so, all features will be transformed into the range [0,1] meaning that the minimum and maximum value of a feature/variable is going to be 0 and 1, respectively.\n\n•\t**StringIndexer:**  encodes a string column of labels to a column of label indices. ... If the input column is numeric, we cast it to string and index the string values.\n\n•\t**Vector:** A vector is similar to an Array. A vector holds multiple number values. In Python, you can do operations on vectors using things like dot product and cross product, in linear algebra. These operations are used to efficiently manipulate data when creating neural networks and 3d rendering.\n\n•\t**OneHotEncoder:** One-hot encoding transforms the values in categoryIndex into a binary vector where at maximum one value may be 1. \nOne hot encoding makes our training data more useful and expressive, and it can be rescaled easily. By using numeric values, we more easily determine a probability for our values. In particular, one hot encoding is used for our output values, since it provides more nuanced predictions than single labels.\n\n•\t**stages:** stages array holds the entities to be processed\n**stages[0]:** --> imputer, convert all blank values in pessanger count with calculated mean statistic\n**stages[1]:** --> assembler and scaler. \n                  assembler holds all numeric columns, and adds them to numeric_features\n                  scaler: scales the numeric_features to scaled_numeric_features column\n**stages[n]:** -->  for each category column\n                    use stringindexer to create corresponding categorycol_index and\n                    use onehotencoder to create corresponding _classVector"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36d96bf5-cbbf-4fba-8980-b4fa62b2522e"}}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml import Pipeline\n\nnumerical_cols = [\"passengerCount\", \"tripDistance\", \"snowDepth\", \"precipTime\", \"precipDepth\", \"temperature\", \"hour_sine\", \"hour_cosine\"]\ncategorical_cols = [\"day_of_week\", \"month_num\", \"normalizeHolidayName\", \"isPaidTimeOff\"]\nlabel_column = \"totalAmount\"\n\nstages = []\n\ninputCols = [\"passengerCount\"]\noutputCols = [\"passengerCount\"]\nimputer = Imputer(strategy=\"median\", inputCols=inputCols, outputCols=outputCols)\nstages += [imputer]\n\nassembler = VectorAssembler().setInputCols(numerical_cols).setOutputCol('numerical_features')\nscaler = MinMaxScaler(inputCol=assembler.getOutputCol(), outputCol=\"scaled_numerical_features\")\nstages += [assembler, scaler]\n\nfor categorical_col in categorical_cols:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_index\", handleInvalid=\"skip\")\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categorical_col + \"_classVector\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n    \nprint(\"Created stages in our featurization pipeline to scale the numerical features and to encode the categorical features.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0f3ed2d-f096-4197-9ec1-014e2833024b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use a `VectorAssembler` to combine all the feature columns into a single vector column named **features**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9baf9f11-ddf0-498b-b19f-dcddd670c655"}}},{"cell_type":"code","source":["assemblerInputs = [c + \"_classVector\" for c in categorical_cols] + [\"scaled_numerical_features\"]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\nprint(\"Used a VectorAssembler to combine all the feature columns into a single vector column named features.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ca21c94-723f-4e4a-8317-1e67fe8d7a85"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Run the stages as a Pipeline**\n\nThe pipeline is itself is now an `estimator`.  Call the pipeline's `fit` method and then `transform` the original dataset. This puts the data through all of the feature transformations we described in a single call. Observe the new columns, especially column: **features**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d565f2f3-f330-45bb-aa2e-58a50202361f"}}},{"cell_type":"markdown","source":["**Pipeline:** A Pipeline is specified as a sequence of stages. Each stage is either a Transformer or an Estimator . These stages are run in order, and the input DataFrame is transformed as it passes through each stage. \n\n**Fit:** Fit function adjusts weights according to data values so that better accuracy can be achieved\n\n**transform():** This method performs fit and transform on the input data at a single time and converts the data points. \nIf we use fit and transform separate when we need both then it will decrease the efficiency of the model so we use fit_transform() which will do both the work."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2342a6e1-022d-4c6a-99af-d9dadf6d8b80"}}},{"cell_type":"markdown","source":["* numerical_cols = [\"passengerCount\", \"tripDistance\", \"snowDepth\", \"precipTime\", \"precipDepth\", \"temperature\", \"hour_sine\", \"hour_cosine\"]  \n* categorical_cols = [\"day_of_week\", \"month_num\", \"normalizeHolidayName\", \"isPaidTimeOff\"]\n\nThis will generate following\n1. Columns numerical_Features and scaled_numerical_Features columns, which will contain the vector containing all numeric columns\n2. A column featurename_index and featurename_classVector for each category column\n3. Features column containing all category_cols\n\nSparse vectors are when you have a lot of values in the vector as zero. \nWhile a dense vector is when most of the values in the vector are non zero.\n\n**Dense vs sparse vectors**  \nMLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values.  \nFor example, a vector (1.0, 0.0, 3.0) can be represented  \n* in dense format as [1.0, 0.0, 3.0] or  \n* in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffb10484-1e28-4b7a-9dcb-bc90423d29be"}}},{"cell_type":"code","source":["partialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n\ndisplay(preppedDataDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de51dc7a-b0cb-4077-837f-c98ec889dc4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Train a multivariate regression model\n\nA multivariate regression takes an arbitrary number of input features. The equation for multivariate regression looks like the following where each feature `p` has its own coefficient:\n\n&nbsp;&nbsp;&nbsp;&nbsp;`Y ≈ β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + ... + β<sub>p</sub>X<sub>p</sub>`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c8ac894-e128-4386-a865-03de29d39667"}}},{"cell_type":"markdown","source":["Split the featurized training data for training and validating the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35f5a2e9-2a02-43e2-80c8-a9e861835a93"}}},{"cell_type":"code","source":["(trainingData, testData) = preppedDataDF.randomSplit([0.7, 0.3], seed=97)\nprint(\"The training data is split for training and validating the model: 70-30 split.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"894b2c1a-9c81-45df-9288-5d183b31ae31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Create the estimator `LinearRegression` and call its `fit` method to get back the trained ML model (`lrModel`). You can read more about [Linear Regression] from the [classification and regression] section of MLlib Programming Guide.\n\n[classification and regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html\n[Linear Regression]: https://spark.apache.org/docs/3.1.1/ml-classification-regression.html#linear-regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5531ccc9-2d4e-4ace-9a7b-5903a8fec927"}}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=label_column)\n\nlrModel = lr.fit(trainingData)\n\nprint(lrModel)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"536675fd-6a5b-4c87-9c28-299a755a2b30"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Exercise 2: Validating a Model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e1bc587-0068-4a4a-8284-d683ec694549"}}},{"cell_type":"markdown","source":["-sandbox\n\nFrom the trained model summary, let’s review some of the model performance metrics such as, Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R<sup>2</sup> score. We will also look at the multivariate model’s coefficients."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f61db909-371d-4ab1-b41e-906b1d00be88"}}},{"cell_type":"markdown","source":["##Important Terms##\n**Root Mean Square Error (RMSE):** Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are\n\n**Mean absolute error (MAE):** Mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.\n\n**R-square (R<sup>2</sup>):** The most common interpretation of r-squared is how well the regression model fits the observed data. For example, an r-squared of 60% reveals that 60% of the data fit the regression model. Generally, a higher r-squared indicates a better fit for the model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21aa1584-dc5b-4a03-ad15-b86d0281cf75"}}},{"cell_type":"code","source":["summary = lrModel.summary\nprint(\"RMSE score: {} \\nMAE score: {} \\nR2 score: {}\".format(summary.rootMeanSquaredError, summary.meanAbsoluteError, lrModel.summary.r2))\nprint(\"\")\nprint(\"β0 (intercept): {}\".format(lrModel.intercept))\ni = 0\nfor coef in lrModel.coefficients:\n  i += 1\n  print(\"β{} (coefficient): {}\".format(i, coef))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dea0276-37b3-4f33-9de4-f03437b0bae6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Evaluate the model performance using the hold-back  dataset. Observe that the RMSE and R<sup>2</sup> score on holdback dataset is slightly degraded compared to the training summary. A big disparity in performance metrics between training and hold-back dataset can be an indication of model overfitting the training data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9dbd05f-b72f-4411-8c85-03d0519e14c9"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\npredictions = lrModel.transform(testData)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"mae\")\nmae = evaluator.evaluate(predictions)\nprint(\"MAE on test data = %g\" % mae)\nevaluator = RegressionEvaluator(\n    labelCol=label_column, predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R2 on test data = %g\" % r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53a8569d-acad-4945-b8c9-0084e314ac57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Compare the summary statistics between the true values and the model predictions**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecc57e50-b579-48fa-b09e-8ac2fa8b4418"}}},{"cell_type":"code","source":["display(predictions.select([\"totalAmount\",  \"prediction\"]).describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfe7ef89-4f5b-4e39-8af0-763a9e2ceb55"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Visualize the plot between true values and the model predictions**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e65866b2-f4bd-4647-8335-38645a7e6ef2"}}},{"cell_type":"code","source":["p_df = predictions.select([\"totalAmount\",  \"prediction\"]).toPandas()\ntrue_value = p_df.totalAmount\npredicted_value = p_df.prediction\n\nplt.figure(figsize=(10,10))\nplt.scatter(true_value, predicted_value, c='crimson')\nplt.yscale('log')\nplt.xscale('log')\n\np1 = max(max(predicted_value), max(true_value))\np2 = min(min(predicted_value), min(true_value))\nplt.plot([p1, p2], [p1, p2], 'b-')\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.axis('equal')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8907456-ae01-41e7-b0ec-579a02d66472"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.0 Train and Validate ML Model_WithInstructions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3234908430213249}},"nbformat":4,"nbformat_minor":0}
